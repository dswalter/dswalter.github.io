---
layout: post
title: Best things of 2015
modified: 2015-12-31
categories: blog
comments: true
tags:
image:
    feature:
---


Some things I appreciated in 2015:

arXiv papers:

Batch normalization- another wonderfully simple idea. We're known for years that neural networks respond better to input data that follow certain distributions. Since the output of each layer is the input to the next, it's a good idea to ensure that each layer is getting "well-behaved" data?

Ladder Networks

Deep residual networks- We begin the "truly deep" deep learning era with this ImageNet-winning paper from Microsoft Research Asia that uses a convolutional neural network with 153(!) layers. Most layers in the network are relatively compact, which makes training more feasible, and an identity mapping is added to help keep the signal strong through all the layers. The results speak for themselves.

Text Understanding from Scratch:
Using words as the building blocks of NLP for so long may partially be a result of much work being done in English (English is not a morphologically rich language, so a lot of our meaning is communicated using noun phrases and verb phrases). But a convolutional neural network trained *at a character level* proves very effective and solving NLP tasks. It's also fun to see Yann LeCun, who is primarily famous for his deep learning work on computer vision, involved in some pretty serious work on text.
http://arxiv.org/pdf/1502.01710v1.pdf



Tools:
Jupyter notebooks (I had been late to the party on these)

t-SNE t-SNE is one of my favorite data visualization methods, and the scikit-learn implementation of t-sne makes use of a full similarity/distance matrix, making it colossally memory-inefficient on many cases. Using this utility



General data science blog posts:
Trey causey's Hiring post.
A/B data science
the hardest part of data science (the most difficult things we do are epistomological) http://yanirseroussi.com/2015/11/23/the-hardest-parts-of-data-science/

Books:
MLaPP
Data Science from Scratch (a nice refresher with very few dependencies)

Technical blog posts:

Deep Dream

The single greatest argument for COBOL in the language wars.
Bot or not on Twitter (close to my heart): http://www.erinshellman.com/bot-or-not/

Karpathy's The Unreasonable Effectiveness of Recurrent Neural Networks

The past few months have seen an explosion of generative models for basically everything: robotrump, etc. Samim (find the name)'s post has some great videos on stuff with drawing. https://medium.com/@samim/assisted-drawing-7b26c81daf2d#.q9axsv7nm

Some people to follow:
T
