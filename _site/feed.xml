<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
<title type="text">Subjective functions</title>
<generator uri="https://github.com/jekyll/jekyll">Jekyll</generator>
<link rel="self" type="application/atom+xml" href="http://dswalter.github.io/feed.xml" />
<link rel="alternate" type="text/html" href="http://dswalter.github.io" />
<updated>2015-11-29T09:22:28-05:00</updated>
<id>http://dswalter.github.io/</id>
<author>
  <name>Daniel Walter</name>
  <uri>http://dswalter.github.io/</uri>
  <email>ds$mylastname@theemailserviceprovidedbygoogle.com</email>
</author>


  

<entry>
  <title type="html"><![CDATA[Overfitting, Regularization, and Hyperparameters]]></title>
  <link rel="alternate" type="text/html" href="http://dswalter.github.io/blog/overfitting-regularization-hyperparameters/" />
  <id>http://dswalter.github.io/blog/overfitting-regularization-hyperparameters</id>
  <updated>2015-11-29T00:00:00-00:00</updated>
  <published>2015-11-29T00:00:00-05:00</published>
  
  <author>
    <name>Daniel Walter</name>
    <uri>http://dswalter.github.io</uri>
    <email>ds$mylastname@theemailserviceprovidedbygoogle.com</email>
  </author>
  <content type="html">
    &lt;p&gt;(This lays the ground work for the next post, which was getting too long to be effective.)&lt;/p&gt;

&lt;h3 id=&quot;overfitting&quot;&gt;Overfitting&lt;/h3&gt;

&lt;p&gt;One of the goals of machine learning is generalizability. A model that only works on the exact data it was trained on is effectively useless. Let’s say you’re tasked with creating a bird-recognition system. If you train a model to recognize pictures of birds, and it gets 100% accuracy on the 130 pictures of 10 classes of birds you showed it, is it a good model?&lt;/p&gt;

&lt;p&gt;It might be, but it depends on how the model performs on images it was not trained on. If performance measures on both the training set and the test set are good, then the model is performing well. But if performance on the test set is bad, then that model is not particularly useful. Through some mechanism, your model has ‘memorized’ the 1300 pictures you showed it. You already had those pictures, so you don’t gain anything by using the model.&lt;/p&gt;

&lt;p&gt;In an ideal world, your algorithm would be able to to discriminate between different types of birds, even if the pictures are very different from the examples it has seen before. To do that, it needs to identify features of those birds that are constant for each bird rather than features of a particular picture or photography session. There are a number of ways an algorithm might try to characterize a Blue-Faced Parrot Finch from this picture below, for eample. One good feature might be to recognize that the Blue-Faced Parrot Finch has an area on its face that is blue.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/9/9e/Blue-faced_Parrotfinch.jpg/1024px-Blue-faced_Parrotfinch.jpg&quot; alt=&quot;&quot; /&gt;
&lt;a href=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/9/9e/Blue-faced_Parrotfinch.jpg/1024px-Blue-faced_Parrotfinch.jpg&quot;&gt;credit for this image: Wikipedia user Nrg800&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We might want to check another picture to see if this feature is consistently present.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://www.cliftonfinchaviaries.org/fsa/blue/bf9.JPG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.cliftonfinchaviaries.org&quot;&gt;credit for this image:  www.cliftonfinchaviaries.org&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;So far so good. This picture of a Blue-Faced Parrot Finch also has a blue face; it seems like a good feature to use to characterize this bird. But what if the algorithm identified a different feature? In the first picture, the bird stood on a forked branch. If ‘forked branch’ was one of the features it used to identify this bird, it would not have generalized to all pictures of this bird. Or, in both of these pictures, the bird is facing to the right; the algorithm might decide that facing to the right is a defining characteristic of this finch. In machine learning, we call these cases &lt;em&gt;overfitting&lt;/em&gt;: an algorithm learns non-generalizable characteristics only present in the training data.&lt;/p&gt;

&lt;p&gt;Overfitting is one of the hidden specters of the field. As the complexity of your model increases, the potential for overfitting increases as well. In this illustration below, the blue line represents the error rate on the training data, and the red line represents the error rate on the testing data.
&lt;img src=&quot;http://www.richardcorbridge.com/wp-content/uploads/2013/09/Overfitting.png&quot; alt=&quot;http://www.richardcorbridge.com/wp-content/uploads/2013/09/Overfitting.png&quot; /&gt;
&lt;a href=&quot;www.richardcorbridge.com&quot;&gt;h/t to Richard Corbridge for this clean overfitting graphic&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;As the representation capacity of your model increases, it is better able to capture variation in the training data, so the blue line monotonically decreases. The red line (testing data error rate) follows that trajectory initially: the model is learning things that are generalizable. But at some point, the model starts to learn features specific to the training data (like our forked branch above), and performance on the testing data begins to suffer: the test data isn’t exactly like the training data, so the ‘memorized’ features hurt generalizability.&lt;/p&gt;

&lt;h3 id=&quot;regularization&quot;&gt;Regularization&lt;/h3&gt;

&lt;p&gt;But complex problems require complex algorithms that can make very subtle distinctions. Right now, almost all the fun tasks (computer vision, question answering, speech recognition, etc.) fall under that umbrella. If we must use a complex algorithm, how can we avoid overfitting?&lt;/p&gt;

&lt;p&gt;The simplest way to avoid overfitting is to give the algorithms too much data to overfit on. By overwhelming the algorithm with data, you force it to decide what is important.  An algorithm that only memorizes the most recent examples it has seen will be exposed by poor performance on the test set. An algorithm that is making wise choices about what is important for it to learn and what isn’t, will improve when given tons of data. For most supervised learning problems, however, labeled data is often prohibitively expensive, so this solution isn’t always feasible.&lt;/p&gt;

&lt;p&gt;Another way to do that is to use a technique called &lt;a href=&quot;https://en.wikipedia.org/wiki/Regularization_%28mathematics%29&quot;&gt;&lt;em&gt;regularization&lt;/em&gt;&lt;/a&gt; to keep overfitting at bay. Without going into details, this technique introduces a complexity penalty that “punishes” the machine learning algorithm for letting the parameters get too large (which usually means overfitting). That is to say, it incorporates a mechanism in the algorithm itself which restricts parameters of the algorithm to make it learn in a way we think is less likely to overfit. Regularization is wildly popular, especially in situations where the data is high-dimensional (lots of different variables).&lt;/p&gt;

&lt;h3 id=&quot;hyperparameter-optimization&quot;&gt;Hyperparameter Optimization&lt;/h3&gt;

&lt;p&gt;When introducing a regularization method, you have to decide how much weight you want to give to that regularization method. You can pick larger or smaller values for your complexity penalty depending on how much you think overfitting is going to be a problem for your current use case. This exposes one of the open secrets of machine learning: the goal is to get the computer to learn how to make decisions automatically, but there are values (like the size of the complexity penalty) impacting performance that must be chosen.&lt;/p&gt;

&lt;p&gt;Every machine learning algorithm has these values, called &lt;em&gt;hyperparameters&lt;/em&gt;. These hyperparameters are values or functions that govern the way the algorithm behaves. Think of them like the dials and switches on a vintage amplifier.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://bb3blog.files.wordpress.com/2012/06/san-505-copy.jpg&quot; alt=&quot;fewer hyperparameters than the average deep neural network&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There are different combinations of amp settings that are better suited to produce different types of sounds; similarly, different configurations of hyperparameters work better for different tasks. Hyperparameters include things like the number of layers in a &lt;a href=&quot;http://ufldl.stanford.edu/tutorial/supervised/ConvolutionalNeuralNetwork/&quot;&gt;convolutional neural network&lt;/a&gt; or the number of neighbors used in a &lt;a href=&quot;https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm&quot;&gt;nearest neighbor classifier&lt;/a&gt;, and they can have a massive impact on how the algorithm performs. Once, I used latent dirichlet allocation &lt;a href=&quot;http://ai.stanford.edu/~ang/papers/nips01-lda.pdf&quot;&gt;(a topic modeling algorithm)&lt;/a&gt; as part of a classification task, and I found that by changing the \(\alpha\) and \(\beta\) parameters, the prediction accuracy on my test set could vary from 0.04 to 0.41. That’s an order of magnitude of difference based on fiddling with two dials.&lt;/p&gt;

&lt;p&gt;Finding the best combination of hyperparameters is called &lt;em&gt;hyperparameter optimization&lt;/em&gt;; it is almost impossible to beat state of the art methods without performing hyperparameter optimization. But there are some subtle dangers. Using one algorithm “out-of-the-box” and laboriously tuning hyperparameters for another example leads to an unfair comparison: in general, hyperparameter optimization squeezes out better performance. A better algorithm will in general outperform a worse algorithm, but sometimes, you can find the perfect combination of hyperparameters, which will allow the best-case version of the lesser algorithm to beat an average version of the better algorithm. Choosing the best hyperparameters is like playing with the dials of one amp until you find the perfect sound; It’s not really fair to compare the sound of a perfectly-adjusted amplifier with one you use default settings on.&lt;/p&gt;

&lt;p&gt;And most vexingly, hyperparameter optimization can lead to overfitting: if a researcher runs 400 experiments on the same train-test splits, then performance on the test data is being incorporated into the training data by choice of hyperparameters. This is true even if regularization is being used! With each time an algorithm is evaluated on the test data, that test data becomes less useful as an “unsullied” evaluator of performance. By the 400th or 4000th evaluation, the test data holds very little mystery and is no longer functioning as a test dataset; it has become a secondary training set.&lt;/p&gt;

&lt;p&gt;There are some strategies that attempt to mitigate this problem: one is to use a train-validation-test split, where the hyperparameters are tuned based on performance on the validation set. This leads to potential concerns about overfitting to the validation set, but the test set is left more “intact” is cross-validation, where the data are split into \(n\) train-test sets. But this mostly just shares the overexposure problem with a larger test dataset (since everything is eventually tested rather than a small subset). But that sharing only postpones the inevitable. The problem still remains that exposure means evaluation is performed on a decreasingly ‘unsullied’ test set.&lt;/p&gt;

&lt;h3 id=&quot;wrap-up&quot;&gt;Wrap-up&lt;/h3&gt;
&lt;p&gt;In the attempt to move toward generalizable machine learning, we try to find algorithms that perform well on training data and testing data, using regularization to pursue that goal wherever possible. And, in general, if two algorithms achieve the same performance on a task, the one with less hyperparameter optimization is generally preferable.&lt;/p&gt;

&lt;p&gt;Machine learning competitions that reward only the single highest-performing team provide a bit of a mixed bag, then. It’s typically impossible to win those competitions without a great algorithm and some hyperparameter optimization. But it becomes difficult to separate the gains that come from better algorithms and the gains that come from more judicious hyperparameter choices. I’ll get into this a bit more with my next post.&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;http://dswalter.github.io/blog/overfitting-regularization-hyperparameters/&quot;&gt;Overfitting, Regularization, and Hyperparameters&lt;/a&gt; was originally published by Daniel Walter at &lt;a href=&quot;http://dswalter.github.io&quot;&gt;Subjective functions&lt;/a&gt; on November 29, 2015.&lt;/p&gt;
  </content>
</entry>


  

<entry>
  <title type="html"><![CDATA[Machine Learning is Magic]]></title>
  <link rel="alternate" type="text/html" href="http://dswalter.github.io/blog/machine-learning-is-magic/" />
  <id>http://dswalter.github.io/blog/machine-learning-is-magic</id>
  <updated>2015-10-02T00:00:00-00:00</updated>
  <published>2015-10-02T00:00:00-04:00</published>
  
  <author>
    <name>Daniel Walter</name>
    <uri>http://dswalter.github.io</uri>
    <email>ds$mylastname@theemailserviceprovidedbygoogle.com</email>
  </author>
  <content type="html">
    &lt;p&gt;Perhaps you have heard of the the sophisticated, elegant &lt;a href=&quot;https://github.com/panicsteve/cloud-to-butt&quot;&gt;Cloud-to-Butt plugin&lt;/a&gt;, which searches through the text on a website for the phrase “The Cloud” and replaces it with “My Butt”. &lt;a href=&quot;http://gizmodo.com/the-best-of-cloud-to-butt-the-only-extension-youll-eve-1685863609&quot;&gt;This compilation&lt;/a&gt; shows some of the most urbane results.&lt;/p&gt;

&lt;p&gt;It struck me that if you replaced “Machine Learning Algorithm” with “Magic Spell” in many contexts, the content would feel very similar.&lt;/p&gt;

&lt;p&gt;So I put together a Chrome plugin called “Machine Learning is Magic” that you can  &lt;a href=&quot;https://chrome.google.com/webstore/detail/machine-learning-is-magic/miejfpgjbmgjkhdflhkdnnkhmpknibmc?hl=en&quot;&gt;download here.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/dswalter/machine-learning-is-magic/master/images/machine-learning-screenshot.PNG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;becomes&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/dswalter/machine-learning-is-magic/raw/master/images/magic-screenshot.PNG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And you get fun intro sites like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/a-tour-of-magic-spells.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I ended up going a bit overboard and doing a few more replacements: “data” becomes “Raw Magic Material”, “probability” becomes “The Fates”, etc. I had fun with it.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/dswalter/machine-learning-is-magic&quot;&gt;The code&lt;/a&gt; is a bit janky, so pull requests are welcome. And if you think I should make the simpler version that only replaces “machine learning algorithm” with “magic spells”, leave a comment. Enjoy!&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;http://dswalter.github.io/blog/machine-learning-is-magic/&quot;&gt;Machine Learning is Magic&lt;/a&gt; was originally published by Daniel Walter at &lt;a href=&quot;http://dswalter.github.io&quot;&gt;Subjective functions&lt;/a&gt; on October 02, 2015.&lt;/p&gt;
  </content>
</entry>


  

<entry>
  <title type="html"><![CDATA[Talking Talking Machines]]></title>
  <link rel="alternate" type="text/html" href="http://dswalter.github.io/blog/talking-talking-machines/" />
  <id>http://dswalter.github.io/blog/talking-talking-machines</id>
  <updated>2015-09-17T00:00:00-00:00</updated>
  <published>2015-09-18T00:00:00-04:00</published>
  
  <author>
    <name>Daniel Walter</name>
    <uri>http://dswalter.github.io</uri>
    <email>ds$mylastname@theemailserviceprovidedbygoogle.com</email>
  </author>
  <content type="html">
    &lt;p&gt;&lt;a href=&quot;http://www.thetalkingmachines.com/&quot;&gt;Talking Machines&lt;/a&gt; is currently THE machine learning podcast. Last fall when it started out, I was intrigued that someone was starting a podcast focusing specifically on machine learning. But I didn’t really listen until this summer, which was my loss. Now that I’m catching up on it, I think it’s terrific. &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://static1.squarespace.com/static/54a56ccbe4b0ab38fed9fc81/t/54a56d1fe4b0c309d01404ce/1442025843814/?format=1500w&quot; alt=&quot;talking machines logo&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As an completely biased person, I think everyone should listen to it; but I do unabashedly love the subject material. The initial segment of each episode is usually a clear introduction to a common ML concepts, the likes of which you might encounter in an introductory ML class. Then the interviews touch on a wide range of the interviewee’s research focus, work, and thoughts on the field. I’m not sure if it’s interesting and fun as a cold-start introduction to the material, but as someone who works in the field, I’m firmly in their target audience. At times, it’s almost a water-cooler discussion for the ML set.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.seas.harvard.edu/directory/rpa&quot;&gt;Dr. Ryan Adams&lt;/a&gt;, a Cambridge-trained ML researcher, has a good combination of breadth and depth of knowledge in the field, and his insights guide the discussion to some interesting places.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.katherinelgorman.com/&quot;&gt;Katherine Gorman&lt;/a&gt; brings an air of professionalism to the podcast. I imagine it’s primarily her editing that keeps the final product smooth to listen to.&lt;/p&gt;

&lt;p&gt;And the guests have been fantastic. I knew they were going to have great interview subjects when the first teaser was the Deep Learning Conspiracy (Hinton, LeCun, and Bengio). Ilya Sutskever’s interview was insightful, as was Hanna Wallach’s where she told the history of the &lt;a href=&quot;http://wimlworkshop.dreamhosters.com/&quot;&gt;Women In Machine Learning (WIML)&lt;/a&gt; conference.  I’m not friends with Geoff Hinton, but Talking Machines gives me a glimpse into his more nuanced opinions on what’s going on ML. Adams has the cachet to get prominent researchers to join the podcast; I imagine it helps that there are so few venues for these researchers to talk about their to listeners who actually care.&lt;/p&gt;

&lt;p&gt;They’ve launched &lt;a href=&quot;https://www.kickstarter.com/projects/487384857/tote-bag-productions-talking-machines&quot;&gt;a kickstarter&lt;/a&gt; to fund their second season. I follow a pretty simple rule for Kickstarter-type-things: do I want to live in a world where this product exists?&lt;/p&gt;

&lt;p&gt;I do, so I’m backing the second season.&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;http://dswalter.github.io/blog/talking-talking-machines/&quot;&gt;Talking Talking Machines&lt;/a&gt; was originally published by Daniel Walter at &lt;a href=&quot;http://dswalter.github.io&quot;&gt;Subjective functions&lt;/a&gt; on September 18, 2015.&lt;/p&gt;
  </content>
</entry>


  

<entry>
  <title type="html"><![CDATA[Why Blog?]]></title>
  <link rel="alternate" type="text/html" href="http://dswalter.github.io/blog/why-blog/" />
  <id>http://dswalter.github.io/blog/why-blog</id>
  <updated>2015-08-18T00:00:00-00:00</updated>
  <published>2015-08-12T00:00:00-04:00</published>
  
  <author>
    <name>Daniel Walter</name>
    <uri>http://dswalter.github.io</uri>
    <email>ds$mylastname@theemailserviceprovidedbygoogle.com</email>
  </author>
  <content type="html">
    &lt;p&gt;A question anyone starting a blog in 2015 should answer:  &lt;/p&gt;

&lt;h5 id=&quot;why&quot;&gt;Why?&lt;/h5&gt;

&lt;p&gt;Like a lot of people, I think things through more throughly and with more discipline when writing them down. If I just consider a topic, I can tend to get into habits of thought. So the act of writing things out tends to give me clearer opinions on a topic.&lt;/p&gt;

&lt;h5 id=&quot;why-not-not-keep-a-journal-then&quot;&gt;Why not not keep a journal, then?&lt;/h5&gt;

&lt;p&gt;That’s a good idea, and I do, sometimes. But I don’t challenge myself nearly as well as other people do. Having a blog where I write out my opinions puts me in a place where other people can bring their perspectives to bear. And I have a few ideas I’d like to put out there.&lt;/p&gt;


    &lt;p&gt;&lt;a href=&quot;http://dswalter.github.io/blog/why-blog/&quot;&gt;Why Blog?&lt;/a&gt; was originally published by Daniel Walter at &lt;a href=&quot;http://dswalter.github.io&quot;&gt;Subjective functions&lt;/a&gt; on August 12, 2015.&lt;/p&gt;
  </content>
</entry>

</feed>